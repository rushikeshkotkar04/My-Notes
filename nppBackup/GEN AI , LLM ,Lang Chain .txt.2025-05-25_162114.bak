GenAI:
	-Models used in Deep Learning
		1.Descriminative Models:
			-Tasks like classification,prediction,regression
			-Trained on labeled dataset
			
			
		2.Genrative AI:
			-Tasks line generating new data 
			-generate new data trainied on Huge dataset
			
			-Models use in GEN AI:
				1.LLM: Large language model
				2.LIM:Large Image model
		
	-LLM Models:
	
	
		
RAG (Retrieval Augmented Generation):
	-If we have Data Source eg PDF:
		-If we wanna make PDA as Documented QnA Application
	
	Modules to create the Applications like this:
		1.Module 1
			1.Data injection:
				-To get data from various sources like
				1.PDF,2.URLs,3.JSON,4.Imgs,5.Videos
			
			2.Split/Devide Data into smaller chunks
				-Split into:
					1.Text
					2.Documented
				-Some LLMs model has limitation on context size that why it is imp
				
				-Techniques to split data:
					1.
			
			3.Embde(Convert text to vectors)
			
			4.Store the Converted Vectors from text into store:
				-VectorStore DB
					1.FAISS DB
					2.CHROMADB
					3.ASTRADB
			
			
		2.Module 2 (Retrive)
			-Retrieval chain
				-A interface to query VectorStoreDB
			
		
		
LangChain ecosystem:
	-A framework to use to build GEN AI Application with any LLM model
	-All model in langChain are open source
	-LangSmith:
		-It has LangSmith help to do MLOPs things eg Debugging,Playground,evaluation,Annotaion,Monitoring
	-LangServe:
		Create chain as RestAPI and helps to tasks like Deployment
				

LangChain:
	-Imp Component of LangChain:
		-Data Injection
			1.Document loader:
				-Different ways to load data from various sources
			
		-Splitter:
			-we can split by:
				-Recursively by characters
				-by Htmlheader
				-Json
				-Charecher Text
				
					-Sample single systax here we can use more
						-Syntax:
							text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)
							final_documents=text_splitter.create_document(var_name)
							final_documents=text_splitter.split_documents(docs)
					
		-Embedding:
			-Converting text into vectors
			Ways To do Embed texts to vectors:
				1.OpenAi:
					syntax:
						-we need langchain-openai for openai api key
						from langchain_openai import OpenAIEmbeddings
						embeddings=OpenAIEmbeddings(api_key=OPEN_API_KEY,model="text-embedding-3-large")
						print(embeddings)
						
						text="This is tutorial in OPENAI embeddings"
						embeddings.embed_query(text)
						
						
				2.OLLAMA
				3.Hugging Face
		
		
		-Retrieval chain/documnet chain:
			-A interface ,Helps to retrive from vector Store and give output
			-we can use the retreiver to dynamically select the most relevant documents and pass 
				those in for a given question
			-Synatx
				-from langchain.chains.combine_documents import create_stuff_documents_chain
				-create_stuff_documents_chain is to give a paragaph or stuff in the document where we can get data
			
			-Flow:
				Input->Retriever->VectorStoreDB
			
			create_stuff_documents_chain: 
				This helper reads the pages you give it and uses the LLM (like ChatOpenAI) to write an answer based on those pages.
		
			create_retrieval_chain:
				This helper combines the library (your vector database, like Chroma or Pinecone) with the page-reader helper to first find the right pages and then answer the question.
LLM:
	-Large language model
	-We have top import model
	llm=ChatOpenAI(model=model_name)
	input=input()
	llm.invoke(input)

Prompt template:
	-How you want LLM model to behave
	-In short setting up the rule how it behaves to give output
	from langchain_core.prompts import ChatPromptTemplate
	
	-Syntax:
	
		prompt=ChatPromptTemplate.from_messages(
		  [
			("system","Your are an expert AI engineer.Provide me answers based omn the question"),
			("user",f"{input}")
			
		  ]
		)
		
		-How to make sure our LLM works
		
		
		-For own chat prompt template
			-from langchain_core.prompts import ChatPromptTemplate
	
	
	
	-How to use prompt template with llm model:
		-now it chains to the llm
		-Syntax:
			chain=prompt|llm
			response=chain.invoke({"input":"Can you tell me LangSmith"})
			print(response)
			
		-in this syntax the value of input goes in the input filed of the prompt template
		-now it first goes to prompt an dthen goes to llm like that


-Output parser:
	-To display msg
	
	1.StrOutputParser:
	
		from langchain_core.output_parsers import StrOutputParser
		output_parser=StrOutputParser()
		chain=prompt|llm|output_parser
		response=chain.invoke({"input":"Can you tell me LangSmith"})
		print(response)
	-we made a chain of where it is like our input goes fron the first then its output go to second and then thord and go on


-Message History : 
	-To make model stateful
	-To store privous convo
		
		
	## Message History : To make model stateful
	from langchain_community.chat_message_histories import ChatMessageHistory
	from langchain_core.chat_history import BaseChatMessageHistory
	from langchain_core.runnables.history import RunnableWithMessageHistory

	store={}
	def get_session_history(session_id:str)->BaseChatMessageHistory:
	  if session_id not in store:
		store[session_id]=ChatMessageHistory()
	  return store[session_id]
	  
	with_message_history=RunnableWithMessageHistory(model,get_session_history)
	
	
	config1={"configurable":{"session_id":"chat2"}}

	resonse=with_message_history.invoke(
	  [
		HumanMessage(content="Whats my name?"),
	  ],
	  config=config1
	)
	resonse.content




tream_messages:
	To let know how many last messages to take forward else delete all from history
	we need RunnablePassThrough class to achieve this

temerature:
	we set it into 0-1
	-creativity depends on value
	


TOola sna Agents:
	Tools to do a specific task and Agent is like to find out the appropriate tool for that task


	In super short:
	Tools = Do the task

	Agents = Figure out what task to do, pick the right tool, and do it

	So an Agent is like a smart assistant that:

		Understands the goal

		Chooses the right tools 

		Uses them in the right way 
	
	
	How to create tools:
		Step1: Import Query Runner or specific thinhs
			from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun
			from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper
		
		
		Step2.1:Using InBuilt tools
			Create api wrapper of the tool and the use that wrapper for query run like following
			
			## Used the inbuilt tool of wikipedia
			api_wrapper_wiki=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)
			wikipedia=WikipediaQueryRun(api_wrapper=api_wrapper_wiki)
			wikipedia.name
			
			## Used inbuit tool for Arxiv
			api_wrapper_arxiv=ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=250)
			arxiv=ArxivQueryRun(api_wrapper=api_wrapper_arxiv)
			arxiv.name
			
			we can also create list of tools for user just to iterate
			like:
				tools=[wikipedia,arxiv]
			
		Step2.2:Create custom tools
			same like we split data and store in vectorDb
			also we have to convert it into tools
			we have to use retriever of the vectorDb and
			using langchain.tools
			retriever=vectordb.as_retriever()
			retriever
			from langchain.tools.retriever import create_retriever_tool
			retriever_tool=create_retriever_tool(retriever,"Langsmit-search","Search any info about langsmit")
			
			
		Step3:Create agent
			Agent is for run the appropriate tools 
			## Agents
			from langchain.agents import create_openai_tools_agent
			agent=create_openai_tools_agent(llm,tools,prompt)
			agent
		
		Step4:Run agent
			we use 
			agent.invole(query)
			
			for running and getting output

Summerize Text:
	Ways:
		1.Stuff:
		2.Map-Reduce: For larger files 
		3.Refine:
	
	1.Stuff Documents chain summerization:
		Most basic type of summerizarion technique
		we user all data as input to the prompt and pass to the prompt
		pass that prompt to the llm and get summerization
		
		problems:
			1.if we have 10 diffent documents in a pdf all gets compbined and summerize it that is problem
			
			also if combined doc gets very big cant give to LLM because of context size limitations of LLM model
	
	2.Map-Reduce summerization technique:
		Better than stuff Document
		
		-instead of combine all documents and give to LLMs
			we devide all insto smaller chunks and pass into a prompt template and get the summerize
			and get each part sumeeries
			we combune all summeries and get final summery
	
	3.Refine Summerization techinque:
		as document is in chunks
		we take single chunk pass to LLM for summerization
		for second chunk before sending this to the prompt
		we take befor summerization of last chunk and pass with second chunk to LLM
		like this we do with all chunks



FineTuning:
	
	Different modules:
		1.Quantization 
		2.LORA 
		3.QLORA
	
	1.Quantization:
		-Conversion from higher memory format to a lower memory format
		-Imagine we have model called Llama2-70b 
		
		it has 70 billions parameters so it is not  possible to get in laptop and use it
		as it has lot of parameters hard to use as space
		suppose it has fp-32 / 32 bit each paramters  so it is too much
		so we do use quantization to convert that big number in smaller like fp-32/32 bit to fp-8/ 8 bit so easy to use for works
		
		
		1.fully precision/Half precision: : Data-> wts and parameters
			half precision :
				when we convert one size to its half during quantization
				
				example:
					fp-32 / 32 bits to fp-16 /16 bits
					
					
		2.Callibration:	Model Quantization ->Problems
			-In short it is simply converting values from high level to lower for use 
			How we convert fp-32/32 bits into fp-8 or 8 bits
			what mathematical formullea is required for it
			
			Types:
				1.Symmetric Quantization:
				2.Asymmetric Quantization
			
			1.Symmetric Quantization:
				1.Symmetric unsined int8 Quantization
					eg:
						(32-bits numbers)
						Numbers=[0.0,0.1 ..... 1000] 
						
						for convert inti unsined int 8 bit(0-255)
						in short convert numbers ranging 0-1000 to 0-255
						
						
						from 0-1000 to 0-255
						
						formulla to convert 32 bit to 8 bits is
							the number wanna convert devided by scaleFactor with rounded value
							
							formulla= Round(num/scaleFactor)
							
							scaleFactor=(Xmax-Xmin)/(Qmax-Qmin)
							
							
						
							
								0-1000 = X (supposed)
								0-255  = Q  (supposed)
								
								scaleFactor=(1000-0)/(255-0)=1000/255=3.92
							
							Now converion:
								if wanna convert 32 bits of 250 into 8 bits
								
								quantizeValue=round(250/scaleFactor)=round(250/3.92)=round(63.7777)=64
								
								
								
								
			2.ASymmetric Quantization:
				1.ASymmetric unsined int8 Quantization from signed integer 32 bits
					eg:
					(32-bits numbers)
					Numbers=[-20.0,.........1000] 
					
					covernt to unsined int 8 
					Numbers are not evern distrubuted it as in negative it might be left skewed or right skewed
					
					as it wont work same scaleFactor formula it wont be that correct as negative values can make changes
					
					so in this we have to have ZeroPoint
					
					so with same formula we have to get Zeropoint in it and add then use same process as symmetric quantization
								
			
		3.Modes of quantization
			1.Post training quantization
			2.Quantization aware training
			
			1.Post training quantization:(PTQ)
				suppose we have already pretrained model
				
				on pretrained model we apply callibration we takes wts data and then convert into quantized model
				 and use model for any use cases
			
			
				Disadvantages:
					as we apply calibration and then use quantized model the accuracy gets decreased
					
					
			2.Quantization aware training:(QAT)
				-as disadvantaged in PTQ first calibration then quantize model it reduces accuracy here 
				we use
				 -pretrainned model perform quantization the we perform finetunning (for fine tune wetake new training data) and then we train quantize model
			
			
		
		FP-fully precision /single precision
		FP-32  (we can say floating point number)
		
		
	2.LORA:Low Rank adaptation
		-Instead of updating all weight it tracks the changes
		
		-for Full parameter finetuning:
			-Update all model weights
			-Hardware Resourses contraint
		
		-what is use of LORA:
			-Instead of updating all weight it tracks the changes of new weights based on fine tunning
			-After it both old and new weights combinned and we get fine tuned model
			-Tracking weight in diffent place 
			
			-the tracked wt is saved in two smaller mateix used matric decomposition based on the parameter Rank
			 Rank is calculated using transport of matrix and make big tracked wt matrix in two small matrix so instead of big we need small memory
			
			-usually we select rank liek 1,2,8 mostly and it works finr actually rank=8 came with microsoft worked brilliantly
			
			-when to use high Rank: If model wanna learn complex things then u can use high rank or model is not trained not trained to probably interact or perform some behaviour we use high ranks for these complex things we can use it 
			
			-if Ranks keep increase still the no parametres still less than all parameters (prooved manually by increasing by researchers)
			
			-makes downstream task smoother
			
			-we multiply thode two small matrix(i.e decomposed matrix) we get tracked wt full matrix and then we add in old to get finetuned
			
			-This solves hardware resourse contraint problem
			
			formula is simple:
				main wts + trackeswts = main wts + multiply of(two decomposed matrix)
				
			
			
			
				
				
			
		
		
	3.QLORA:Quantized LORA or LORA 2.O
		-in QLora all the things in decomposed matrix convert values into lesser values
		-Ex : convrts fp-32 /32 bits into fp-16/ 16 bits or lesser like
		so we wont require much more memory so ot is again better than LORA
		
		-and best thing is that it has mazing algorithms like as we quantized so small value again back to its big value
		like from fp-16 / 16 bits or less to fp-32 or 32 bits (just last examples used so to understand) 
		
		
		
	PEFT: PARAMETER EFFICIENT TRANFER LEARNING for NLP : LoRA comes under it
		it feezing most of the wts of LLM and some of wts retrained and those helps to accurate results
	
	bitsandbytes: quantization
	
	tranformer 
	
	trl
	
	Full parameter finetuning:
		-Update all model weights
		-Hardware Resourses contraint
	

	
	Techniques of fine tune:
				1.Prefix Embed
				2.Prefix Layer
				3.Adapter
				4.LoRA
				5.QLoRA
	
	
	
	for the fine tune the model:
		Step1: Initialize model and its tokenizer
		Step2: Model to its fineTune technique like lora,qlora or any
		Step3: training arguments / Hyper parameter initialization like optimization and all
		Step4: Use Trainer and apply training arguments and fit the model
		Step5: Check the model accuracy
		Step6: Save the model
		
	
	
	
	for distilbert
		1.call pretrained model and it own tokenizer
		2.convert encoding into dataset objects
		
	
PyTorch:
	To chq is cude working in it or not
		torch.cuda.is_available()
	
	find GPU name:
		torch.cuda.get_device_name(0)
	
	memory allocated:
		torch.cuda.memory_allocated()
	
	Tensor Basics:
		A vectors and metrices also called multidiam arrays
		Term and set of techniques known in ML to train and operation deep learning models 
		
		replacement for numpy
		
	suppose: arr=np.arr([3,4,5,6])
	
	converting numpy arry to tensors:
		tensors=torch.from_numpy(arr)
	
	but in this a problem if we cange any value i tensor it also got changed in arr as both shares same mempry location
	
	so we can use another method to do it
		tensors=torch.tensor(arr)
		
		-it solve the problem previously happends
	
	Arithmematic operation
		a=torch.tensor([2,3,4],dtype=torch.float)
		b=torch.tensor([7,8,9],dtype=torch.float)
		
		a=torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float)
		b=torch.tensor([[5,7],[7,8],[9,11]],dtype=torch.float)
		
		
		Addition:
			a+b / also use torch.add(a,b)
		
		Multiplication and dot operation
			a.mul(b)
			a.dot(b)
		
		for matrix multiplication
			x.matmul(x,y) //  x.mm(x,y) // x@y
		
		
	
	Backpropogation usinh pytorch:
		y=x^2
		
	
	
	Hyper parameters:
		requires_grad=True --> to track changes on specific tensor for future use
		