Mathematics:
	Statastics and it applications:
		
		Types:
			
			1.Descriptive:
				It consists of organizing and summersizing the data
				
				Techniques:
					1.Measure of central tendancy
						Mean
						Median
						Mode
					
					2.Measure of dispursion
						Varianance
						Standerd daviation
						
		
		
			2.Inferential/Conclusion:
				Suppose u collect some data we make some conclusion using some experiments
				gives us some other data and that data called population data
				Exprimnets:
					Z-Test
					T-Test
		
		Population(symbol: N) and sample(symbol :n):
			
			consider we have a village of 100 people and i want to know what is the average weight of peoples in the village
			
			for
				-Population : we have to visit all peoples place and take weight of people
				
				as it is hard to do so we can give sample like some people and get there and conclude on it
				
			
		Measure of central tendancy
			1.Mean (symbol:μ)
				-Average of all elements in data set
				
			
			2.Median:
				Sort the data and gets middle value if odd numbers of element there else if even no is present
				then take middle to after sorting and the averatge of those two element will be median
				
				-Helps to fight against outliers
				
				
			3.Mode:
				
				Most frequent no present
		
		
		
		Measure of dispursion:
			dataset={1,1,2,3}
			Varianance(σ²):
				-It tells you how far the data points are from the mean, on average, but in a squared sense.
				
				Formuulla:
					σ²=sum(((dataele-mean)^2)/total elements))
				
			
	
		
	Equation of line,3D plane and hyperplane(n diamension)
	
	

Data analysis with python:
	
	1.Numpy:
		A mathematical lib helps to perform computation also help with matrix and list and other maths operations on data.
		
		everything in numpy array(np.array(list of elements)) are element wise
		
	
	2.Pandas:
		
		
		1.df[clname]     --> give clm its dtype is series
		2.df.loc[index]  --> give rows 
		3.df.iloc[index] --> give clm 
	
	
	Feature engineering:
		
		1.Handling missing values:
			
			Mechanism why missing values happens:
			
				1.Missing completly at random (MCAR):
					Without reason it happens like might be some error at entry end
				
				2.Missing at Random (MAR):
					Depends on other data why this data is missing
					Because there no data present for other entity
					
					like if you dont have mobile then u wont have mobile number
				
				3.Missing data not at random(MNAR):
					missing is because of some other reason or dependant
				
				
			Techniques:
				
				1:Delete the rows where missing values:
					Disadvantage: We can loose useful data also
					
					syntax to do:
						df.dropna()
				2:Delete clms has Misisng values:
					Disadvantage:Can delete useful clm
						
						syntax to do:
							df.dropna(axis=1) 
								(axix=1 means clm by default its 0 means rows)

				
				Imputation Techniques to handle missing values

					1-Mean Value imputation
						-Replace missing values with the mean of clm values
						-If we have different kind of distrubuted like righr skewed or left skewed it wont work

					2-Median Value imputation
						-It works also if we have different kind of distrubuted
						-It also helps to ahndle outliers in dataset
						
					3.Mode Imputation techinque
						-It is used for categrical clm has null values
					
					4.Random value :
						Randmoly picking value and fill but every run it can be changed
					
		
		2.Handling imbalanced Dataset:
			-In categorical database there might be chance
			some data is present a lot and some less so it get imbalalnced
		
			Techniques:
				1.Upsampling:
					Incease the datapoint from minorities
					
					cons:
						varience is not increaing
				
				2.DownSampling:
					Reduce the datapoints from majority
				
				3.SMOTE (Synthetic minority oversampling Technique)
					-Interpolating techinque
					-Create or join two nearest point and create new datapoint there
					-So variance can be increased
		
		3.Handling outlieres
			
			It can be done by box plot
			-We have to use 
			5 number summary is imp to get box plot:
				min val,max val,median,Q1,Q3,IQR
		
		4.DATA ENCODING
			we can use sklearn.preprocessing
			-As machine only knows the 0/1 language we have to convert categorical data into 0/1 format
				Types:
					1.Nominal or ONE HOT ENCODINH
						-Convert into binary format 1/0
						-It creates the num of clm as many diffent values present there and assign 1 if that clm value there else 0
						-Should not use if there are more values present then it will create as many clm there should not use in those cases
						-Sparse matrix
					2.Label and ordinal Encoding:
						-Assigns unique lebel to each categories
					3.Ordinal encoding:
						-Use to encode data in way of ranking or order
						example:
							Hightschool,College,Graduate,Post-Graduate:
							It will convert them in form like:
								
								Hightschool:1
								College:2
								Graduate:3
								Post-Graduate:4
								
					4.Target Guided Ordinal Encoding:
						Technique which is used to encode categorical variable based on there target variable
						Useful when there are lot of unique categories
					
			
	EDA:
		To be learned
	
	

Machine learning:
	
	Types:
		1.Supervised ML:
		
			Task: To predict house price
			
				we have dataset with features like
					size of housemNo of rooms in housemPrice of house
				
			In supervised ML we devide into Independant/Input and Dependadant/Output feature
			
				Types:
					Regression: Output values will be contineous
					classification:Output are in categories
			
			
			Algorithms used in supervise:
				Linear regression(Regression)
				Ridge and Lasso(Regression)
				Elastic Net(Regression)
				Logistic Regression(classification)
				Decision Tree (both Regression and Regression)
				Random forest (both Regression and Regression)
				Adaboot (both Regression and Regression)
				XgBoost (both Regression and Regression)
				
		
		2.Unsupervised ML:We create clustors based on situation
			
			Task:Customer segmentation
			we do not know the output feature we need to predict anything
			
			features like:
				Salary   spending score
				20000        9
				45000        2
				-- -         -- 
				-- -         --
				-- -         --
				
			lets suppose i launched a product and want to send mail dsicount coupon via main we can create clustor
			
			Algorithms used in Unsupervised:
				K Means
				Hirarichel mean
				DBScan clustering
		
		Reinforcement learning:
			Application will learn by itself by getting some amazin rewards
			
			eg:
	

Natural Language processing:
	
	Step 1:
		Text preprocessing 1: -> Clean the text data 
			
			Techniques:
				Tokenizations
				lemmitization
				stemming
				stopwords
				
	Step 2:
		Text preprocessing 2: -> Convert Text data into vectors
	
			Techniques:
				BOW (Bag of words)
				TF-IDF
				UniGrams
				BiGrams
		
	Step 4:
		Text preprocessing 3: -> Convert Text data into (vectors advance)
	
			Techniques:
				Word2vec
				AvgWord2Vec
	
	Step 4:
		Deep Learning technique used for handling text related use cases like Spam or not yes or no

			Techniques:
				RNN
				LSTM RNN
				GRU RNN
	
	Step 5:
		Text preprocessing 4: -> Convert Text data into (vectors advance)
	
			Techniques:
				word embading
	
	Step 5:
		Transformer
	
	Step 6:
		BERT
	
	
	
	
	
	-Tokenizations:
		
		-A process to convert paragraph into tokens(Sentences)
		and sentences into words
		
		Corpus:     Also calls paragraph
		Documents : Sentences
		vocabulary: Unique words
		Words:      Words same words
		
		we can use NLTK to perform tokenization
		
		for words we use : from nltk.tokenize import word_tokenize
		for sentence we use : from nltk.tokenize import sent_tokenize
		
	
	-Stemming -Text Preprocessing:
		Stemming is process to its stem that 
		stem mean deleting prefix/sufficx of word to convert into same root word
		it may be meaningful or not
		
		example:
			Task : comments on product is a positve review or negative one(classification)
			
			example:
				we have words like eating ,eat,eaten all convert into eat
			
				
			
		Stemming techniques:
			1.PorterStemmer
			2.RegexpStemmer
			3.SnowballStemmer: Better than Porter stemmer (Mostly used)
		NLTK usase RegexpStemmer class
	
	-Lemmatization: 
	
		-It makes word meaningful after stem it so it makes root word better
		-Techniques:
			WordnetLemmatizer:
				it uses morphy() function to the wordNet CorpusReader to find lemma
			
		in lemmitization we have pos='n' (pos-> Parts of speech)
		it means noun
		for 
		noun->n
		verb->v
		adjective->a
		adverb->r
		
		
		by default pos tag is n (noun)
		-It is slower than stemming
	
	-Stopwords:
		The words like The,Have,And,Of,Their,Two,Why.... are stop words
		wont play imp role in the our logic so these are stop words
		
		we can get all stopwords by using nltk
	
	
	-Converting text into vector:
		
		Technique:
			1.ONE HOT ENCODING
			2.Bag of words (BAG)
			3.TF-IDF
			4.Word2vec
			5.AvgWord2Vec
		
		
		1.ONE HOT ENCODING
			- It creates a clm for every unique word and create matrix
			per documnet as per word present as 1 or if not present then 0
		
		2.Bag of words:
		
			Dataset
			
			  Text                  		o/p
			1.He is a good boy       		  1
			2.she is a good girl     		  1
			3.Boy and girl are good  		  1
			4.Boy is good and girl is good    3
			
			
			Step by Step impl of bag of words:
				Dataset -> convert all the words to lower case -> 
				remove stopwords -> calculate uniqe words with there frequency ->
				keeping words based top most frequence keep in feature ->
				it gets convert into vector if present then 1 or else 0 
				
			step 1: Lower and remove stop words dataset become:
				
				1->good boy
				2->good girl
				3->boy girl good
				4->boy good girl good
			
			step 2: Calculate uniqe words with there frequency
				
				words/vocabulary          frequency
				good                        5
				boy                         2
				girl                        2

			step 3: Apply Bag of Words (keeping words based top most frequence keep in feature)
			
				It creates vector using uniqye word as feature/clm
				and document/corpus/sentence becomes in dataset above mentioned is like
					
					Binary bag of words if present 1 else 0
					    good  boy  girl    o/p
					
					1->  1     1    0       1
					2->  1     0    1       1
					3->  1     1    1       1
					4->  1     1    1       1
					
					
					Normal bag of words it has the frq of word present in it
					
						good  boy  girl    o/p
					
					1->  1     1    0       1
					2->  1     0    1       1
					3->  1     1    1       1
					4->  2     1    1       1
					
					the count wise store the count if each feaure in word
					
			 and after all this this will be trained by ML/DL model
			
			Advantages  :
				-Easy to impl and intitive
				-Input is fixed (helpful for ML algos)
				
			Disadvantage:
				-Sparse matrix or array is still there and it leads to overfitting
				-Ordering of words in vector changed so meaning of word gets changed
				-Out of vocabulary
				-Semantic meaning is still not getting captured
			
		-N-Gram:
			-Combination of words
			
			s1-> The food is good
			s2-> The food is not good
			
			vocabulary :
			
							UniGram      		  BiGram
						food  not  good     food+good   food+not  not+good
				s1->	  1    0    1          1          0         0   
				s2->      1    1    1          0          1         1
			
		
		-TF-IDF:(Term frequence -Inverse document frequency)
			
			Formula for tem frequence:
					
				term freq(TF)=No of repetation of words in sentence/no words in sentence
				
				IDF=  log base e (No of sentence/no od sentences containing the word)
				

				
		
			
			 Text                  		o/p
			1.He is a good boy       		  1
			2.she is a good girl     		  1
			3.Boy and girl are good  		  1
		
			
			1->good boy
			2->good girl
			3->boy girl good
		
			
			calculating term freq(TF) with formula
		
									sentences 
				vocabulary		1      2      3
				
				good            1/2    1/2    1/3
				
				boy				1/2     0     1/3
				
				girl			0      1/2    1/3
				
			
			colculating INVERSE DOCUMENT FREQUENCY (IDF)
			
			
									
				vocabulary		IDF
				
				good             log base e(3/3)=0				
				
				boy				 log base e(3/2)      
				
				girl			 log base e(3/2)
		
		
			we multiply we multiply every TF sentence with everyother IDF
			every row rp everyother row in both TF and IDF formulla
			
			final TF-IDF
			
			                    words
			sentances      good             boy                       girl
				1            0          1/2*log base e(3/2)             0  
				2            0                0                       1/2*log base e(3/2)
				3            0          1/3*log base e(3/2)           1/3*log base e(3/2) 
				
			
			
			Advantages:
				
				-Intutive
				-Fixes size of fetures(Vocabulary size)
				-Word importance is getting captured:
					-if word is present in all the sentenes then it is given a less importance
					-If word is  not in every sentence the we have to give some value to them
			
			Disadvantage:
				-Sparsing is still exists
				-Out of vocabulary
		-Word embedding:
			-a technique that represents words as numbers so that machines can understand them
			-it is a term representation of words for text analysis ,Typically in the form of realvalues vector that encodes
			the meaning of the words such that the words that are close in the vector space are expected to be similar in the menaing.
			
			-in short it is just relationship between two words
			-It converts word into vector and plot on graph checj the distnce between two words 
			-simillar/same meaning words are close to each other and opposite words are away in space of vector
			
			example:
			
				there two words 1.Happy 2.Excited
					-both are same
				but suppose there are words like 1.Happy 2.Angry
					-Both are different
				
			
			there two types of word embading technique
			1.Count or frequency
				types:
					1.OHE
					2.BOW
					3.TF-IDF
			
			2.Deep learning trained model
				types:
					1.Word2vec
						types:
							1.CBOW (continuos bag of words)
							2.Skipgram
							
		
		-Word2Vec:
			-It is trained on 3 billion words by google
			-Value range in between -1 and 1 
			
			
			Advantages of word2 vec:
				-Instead of sparse matrix we get dense matrix
				-Symantic information will get captured (simmilar word captured like synonyms or asynonims)
				-Fixed set of diamensions so we are not dependant on vocabulary size
				-Out of vocabulary problem also solved as we are capturing all info like symantic information and all
				
				
				
			
			Technique for NLP,uses Neural netwok model to learn work associate from large corpus/sentence
			once trained it can detect synanous word or suggest additional words for partial sentence
			
										vocabulary words :
				feature 		|	Boys    Girl    King    Queen    Apple     Mango
				representations |
				                |    -1      1      -0.92     +0.93   0.01      0.05
				Gender          |    0,01    0.02    0.95     +0.96  -0.02      0.02
				Royal           |    0.03     0.02   -0.75     +0.68   0.96      0.96
				Age             |    ..................................................
				.               |    ..................................................
				.               |    ..................................................
				.               |    ..................................................
				nth             |    ..................................................
				
			-Every word in vocabulary converts into feature representation
			-Means each and every vocabulary words makes/creates it own vector
				of feature representation and create its vector it can be different diamentions
			
			Above table has random values but in real the equatin like
			
				
				Kng-Boy+Queen=Girl
			
			we can you 2dVector of a words also like following
			
				King=[0.95,0.96]    Man=[0.95,0.98]
				Queen=[-0.95,0.96]  Women=[-0.94,-0.96]
				
					Kng-Man+Queen=Women
				
				we can have cosine similarity:
				
				  we can use distance formula to chq how away two words on plane
				   the formula is :
						
						distance=1=cos(θ) (θ is angle between two points)
					
					if distance betweentwo word is 0 then word is same
				
			
			
			
			1.CBOW(Continous bag of words):
				
				suppose we have corpus/sentance or paragraph:
				
					Microsoft company is related to data science
					
					
					
					we select as window size ,
					so we sleect window size count of words
					the we select center word
					so input is left side of middle word and right side 
					and middle is output 
					then we move window with one word and select middle and have input and ouput same types
				
				C-BOW: is fully connected neural network
				CBOW -> If we have small dataset we should apply
				
				how to make it more accurate:
					Increase the training data
					Increase the Window size
			
			2.Skipgram:
				
				suppose we have corpus/sentance or paragraph:
				
					Microsoft company is related to data science
				
				everything will be same as CBOW as window size and all but 
				juts input gets as output and output as input
				
				CBOW -> If we have small dataset we should apply
				
				how to make it more accurate:
					Increase the training data
					Increase the Window size
		
		
		
		-AVgWord2Vec:
			-It helpful for text data to convert into vector also
			
								DATSET
					
			    sentance          Text                  O/p
				    1             The food is good       1
					2             The food is bad        0
					3             pizza is amazing       1
			
			By usinh google pretrained word2vec model
			
			Every word of DATASET gets convrted into 300 diamentional vector 
			so if we are getting 300 diamension of a single sentence instead every word in the sentence converted into 300 diamension
			
			so we take all vectors of words and take avg of that vector and store it in sentance vector and perform same for all 300 vectors for every word line wise			




Deep Learning:
	
	We are going to use Tensor Flow,Pytorch
	
	1.ANN : Artificial Neural Networks
		Can solve Classification and Regression problems
		
	2.CNN : Convolutional Neural network (input can be in Images and video frames)
	
		We have RCNN ,MASSed RCNN for object Detection or image segmentation using YOLO
	
	3.RNN : Recurrent Neural network If our usecase is related to NLP
			It solves NLP and Time series problem
			LSTM RNN,GRU RNN,Word Embedding,Bidirectional LSTM RNN,Encoder Decoder
			Transformer,BERT
			
	
	
	Perceptron:[Artificial Neuron or Neural Network Unit]
		
		There are single layerd and multi layed perceptron model
		
		-Single network Neural network
		-Use to solve binary classification
		-It is also a linear classifier
		
				        DATASET
		
			IQ    No.of Study hrs      O/p
			
			95          3               0
			110         4               1
			100         5               1
		
		                                ------
										|Bias|
										 ----|
											 |
											 V	
			INPUTS	  INPUT LAYER          HIDEEN LAYER
						
				x1		-------|     w1     ----|---|                                  
			--------->	|  O   | ---------> |   |   | --------->   		
						|  O   | ---------> |   |   |                                                                                            
			--------->	|------|    w2      |---|---|                                             
				x2
		                    It is single neuron
							in single neuron we perform two task happens 
							step1:z=(Summation of xiwi(every input and weight))+Bias
							step2:we get step1(z) value and we add activation functiom
			
			Importance of Bias: 
				Bias has value to make sure the step1:value not become 0 
				as wights gets added randomly and wights can be 0 
				so we add small bias ans it is called noise
			
			Aim of activation functions:
				It transform the output between some values like (0,1) or (-1,1)
				-Mainly used in binary classification problem
				
	
		Advantages:
			-Linear seperable problems can be easily solved
			
		
		Disadvantages:
			cant solve hard problems
	
	Multi Layerd Perceptron Model:[Artificial Neural Network]
	
		1.Forword Propogation
		2.Backword Propogation
		3.Loss Function
		4.Optimizers
		5.Activation functions
		
		-We have more than one hidden layer
		
		All independant clms have a a input neuron there and go one py one
		alst layer is OUTPUT layer
		Example DataSet
				
			
			IQ		Study Hrs  		Play Hrs 	O/p(pass(1)/Fail(0))
			----------------------------------------
			95        4                 4        1
            100       5                 2        1
			95        2                 7        0
		
		
					Neural networks diagram
					
					Forword propogartion:
					
			--------------------------------------->		
					
			Inputs          Hidden Layers(Neurons)
			layer 			
			
						------    			------   
						|Bias|				|Bias|
						|----|				|----|
							 |                   |
							 |	                 |
			      w1          V                   V
			--->0-----> |---------|   		|---------|
			      w2    |Neuron cn|   w4	|         |
			--->0-----> |be more  |-------> | OUTPUT  |
			      w3    |one neuron|   		|  LAYER  |
			--->0-----> |---------|   		|---------|
			              
						  From this neuron    and From this neuron
						  suppose we get        suppose we get
						  output o1               output o2
		
				  <----------------------------------------
								Backword propogation
		
		
		Forword propogartion:
			-performing some steps:
			-we assign weights (randomly)
			-first we calculate z in it by 
			summation of all wiegths with input feature and bias
			-and then apply activation function in it
			and send the ouput to next hidden layer
		
		Backword propogation:
			
			we go and update weights and send next input to the NN 
			this happens until the loss / error gets minimized for 
			each and evry input against its output
	
		
		
	
	Activation function:
		-helps to activating or deactivating neuron
		1.Sigmoid:
			-convert z value between 0 to 1
			
			formula is:
				
				σ(z)=1/(1+e^(-z))
			
			terms:
				-z is the summation of weights and inputs 
			
			
			Advantages:
				-Suitable for binary classifications
				-Clear predictions
				
				
			
			Disadvantage:
				1.
					In chain rule of deriavtion as sigmoid makes value between 0 to 1
					the darivation can calculate to 0 to 0.25 with sigmoid
					-In chain rule of otjer weights it will affect there inputs and can give less accurate results
					so instead of 0 to 1 we are getting always 0 to 0.25 so can make diffenrce in answer
					as we are adding condition if value <0.5 then False else True
					-In short old weight hardly gets updated and scan make performance slow and less accurate also
					and this is also caled 
					-Vanishing gradient problem
					
					-It is fine for smaller neural networks but we have deeper neural network
						Then it is bad choice
				2.Function out put is not zero centered:
					-If it is zero centered efficient weight updation can be done
					-as it is not zero centered efficient weight updation not happening
				3.pron to vanishing gradient problem
				4.As formula is exponential so it takes time to calculate:Time consuming formulla
				
		
		
		2.Tanh Activation function:
			
			tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))
			
			-It convert values from -1 to +1
			-so darivative of any thing of tanh activating alwats between 0 to 1
				as it convets value from -1 to 1
			-This is hyperbolic tangent function
			-whole function us zero centric so efficient weight updation happens in good way
			-For mediaum size NN u many not face any problem like vanishing gradient
				but for deep NN u can face it
				
			Disadvantage:
				-For mediaum size NN u many not face any problem like vanishing gradient
				but for deep NN u can face it
				-Problem with computation as formula is in exponential form so it may slow the processes
			
		
		
		3.Relu (Rectified Linear Unit) Activation function:
			
			
			Formula:
				ReLu=max(0,z)
				
				whatever answer get by z is answer is bigger than zero else zero
			
			Advantages:
				-Vanishing gradient gets solved
				-darivative of z is either 0 or 1(backword propogation)
				-As activation functions darivation either gets 0 or 1
				 as it is getting 1 then it new weights getting updated with lil good value
				 and process getting increasing
				-Calculations are super fast as formula is easy no exponential terms
				-Has linear relationship
				-Musch faster than sigmoid and tahnh
				 
			Disadvantage:
				
				-in short leads to dead neuron
					As activation functions darivation either gets 0 or 1
					as it is getting zero then it new weights getting same as old then
					further process wont happens everytime it gets same as old so neuron will be
					dead/neuron with same functions or no functionality
				-Relu functions output is either 0,z so it is not zero centric at all either we get zero or positive number
		
		4.Leaky relu or parametric Relu:
			
			-as in mormal Relu we get either 0 or z/max value so to solve this
				we use leaky Relu or parametric Relu
			
			we multiply z with small constant 0.01 to z and keep it as instead 0 in formula
			if α is 0.01 then it is leanky relu if other than 0.01 it is parametric Relu
			formulla:
			
				f(x)=max(α*z,z)
				
				we have to find out α value herecan be find out hyperparametric tuning
				terms:
					α => hyperparameter
					
			
			Advantages:
				-We will never get zero in back propogation as activation functio of z ansewer so it solve deead neuron problem
				- And in has all other advantages of Relu
			
			Disadvantage:
				-Leaky Relu /parametric Relu functions output is either 0,z so it is nit zero centric at all either we get zero or positive number

			
		
		5.ELU(Exponential Linear Unit):
		
			Formulla:
				f(x)=z if z>0 else α*((e^z)-1)
			in backward propogation:
				as darivative of this activation function in backward propogation either 1 if positive or less than 0 
			
			-It is used to solve RELU problems
			
			
			Advantage:
				-Solving dead neuron proble as we wont get darivative of this activation function value as 0
				-0 centric
			
			Disadvantage:
				
				as maths formula is exponential so it is slower as computationaly slow
		
		6.Softmax activation function:
			as output neuron or layer use sigmoid activation function as binary output has single neuron
			lest suppose we have more than one output neuron so it is multi classification not binary
			then as signoid wont work as it only gives 0 or 1 so cant use in such condition
			so in these scenario we can use softmax activation function 
			
			Formula:
				
				softmax=e^yi/sum(e^yk)
					In short we are finding probability of each out neuron
					and max of probability of output neuron is the output
					
				k is range from 0 to no of output node
				yi=> Output of ith output neuron
				i=> it is no of output neuron
				k=> no of output node
				
			how it works:
				lets suppose we get out put of 4 neuron like -1,0,3,5
			
		When to use which activation function :
		
			-for output layer should use sigmoid if binary if multi classification then softmax
			-Relu/ELU/Leaky rely/parametric relue in middle hidden layer: in short Relu or its varient
		
	Weight updation formula
		
		
		Terms :
		 L=loss functions
		 w_i_old=old ith wieght
		 w_i_new=new ith wieght
		 α=>learning rate
		 o1=output1
		 o2=ouput2
		 
		 
		for last weight
		
			w_i_new=w_i_old- α * darivative(L)/darivative(w_i_old)
		
		α=>learning rate 
		terms:
			(darivative loss fun with darivative of weigh_i_new)-> slop of graph with respect to loss and weight
			it creates gradient descent
	Learning rate:
		If learning rate too hight can affect the weight value and may can o outside of actual graph
	
	
	Terms :
		 L=loss functions
		 w_i_old=old ith wieght
		 w_i_new=new ith wieght
		 α=>learning rate
		 o1=output1
		 o2=ouput2
	
	chain rule of derivation for weight updation
		
		while backword propogation weight updation happens from right to left
		
		for last weight updation formula  is quate simple 
		w_i_new=w_i_old- α * darivative(L)/darivative(w_i_old)
		
		exapnd:
			
			w_i_new=w_i_old- α * darivative(L)/darivative(w_i_old)
			
			we can write :
			 darivative(L)/darivative(w_i_old)= (darivative(l)/darivative(o2))*(darivative(o2)/darivative(w_i_old))
		

		for weight 1st, we can calculate like:
			
			w_1_new=w_1_old- α *(darivative(L)/darivative(w_1_old))
			
			
			(darivative(L)/darivative(w_1_old))= (darivative(L)/darivative(o2)) * (darivative(o2)/darivative(o1)) * (darivative(o1)/darivative(w_1_old))

		but for others we have to expand the darivative for other
		
		darivative
		
		
	Loss Function:
		
		it calculate how away predicted output and actual output
		loss function= (y- ŷ)^2
		
		in short it calculate error and out Deep learning model try to reduce the error
		
		and our deep learning model try to minimize it
		
		-we use optimizer to reduce loss 
		

	OPTIMIZER:
		Optimizers help to reduce loss value
		
	
	
	
	Vanishing gradient problem and activation function:
	
		In chain rule of deriavtion as sigmoid makes value between 0 to 1
		the darivation can calculate to 0 to 0.25 with sigmoid
		-In chain rule of otjer weights it will affect there inputs and can give less accurate results
		so instead of 0 to 1 we are getting always 0 to 0.25 so can make diffenrce in answer
		as we are adding condition if value <0.5 then False else True
		-In short old weight hardly gets updated and scan make performance slow and less accurate also
		and this is also caled 
		-Vanishing gradient problem and activation function:
	
	
	What is diffenernce between loss function and cost function
	
	-In loss function we calulate (y- ŷ)^2 for every 
	-In Cost function we calculate summation of  (y- ŷ)^2 for every output
	
	
	Loss Function vs Cost Function:
		
		Loss Function:
			In loss function we take every data point and do forward and backward propogation
		
		Cost Function:
			We pass all data at once to neuron 
			
		
	
	Loss Functions:
		
		Regression:
			1.Mean squared error (MSE)
			2.Mean absolute error (MAE)
			3.Hubor Loss
			4.Root Mean square error (RMSE)
		
		1.Mean squared error (MSE):
			
			Formullas:
			
				Loss function=(y-ŷ)^2 
				Cost function= 1/n * sum_of_1_to_n((yi-ŷi)^2)
			
			Advantages:
				-it is differntial (Any point of time we can find darivation/differentiation)
				-it has 1 local or global minima
				-it convergeous faster
				
			Disadvantage:
				-not robust to outliers: cost function getting increasing and best fit line might wont be at best position
				
		2.Mean Absolute error:
			
			Formulla:
				
				Loss Function= abs(y-ŷ)
				
				Cost Function=1/n * sum_of_1_to_n(abs(yi-ŷi))
		
			-If dataset has outliers we should use mean abosulte errors
			Advantage:
				-Robust to outlieres as we are not squaring the error or difference between actual output and output got from Neural networks
			
			Disadvantage:
				-conversion takes time as we dont use parabola curver instead we use sub-gradient curver

	
		3.Hubor Loss:
			Combination of MSE and MAE
				
				If u have dont have outlier use MSE 
				if u haveoutliers use MAE
	
		
		4.Root Mean squared Error:
			
			Cost Function= root of (summation_of_from_i_to_N((yi-ŷi)^2)/N)
			
			Advantages:
				Assignment for us
				
				
			Disadvantage:
				Assignment for us
				
				
	Loss Functions for Classification problems:
		
		classification:
			Cross Entropy (Loss function)
				Types of Cross Entropy 
				1.Binary Cross Entropy : used for binary classification
				2.Categorical Cross Entropy : Used for Multiclass classification
				3.Sparse Categorical Cross Entropy : Used for Multiclass classification
			
			1.Binary Cross Entropy : used for binary classification:
				
				Loss Fuction=-y*log(ŷ)*log(1-ŷ) 
					log loss formula
					
				if y==0:
					loss function = -log(1-ŷ) 
				if y=1: -log(ŷ) 
			
			2.Categorical Cross Entropy : Used for Multiclass classification
				
				-It takes output clm and apply OHE on it
					
				lossfunction(inpi,outi)=- sum_of_from_i_to_noofcategories(yij*ln(ŷij))
				
				if categories present then yij == 1 else 0
					
					
				i=>All inputs
				j=>All output categoes which are converted into output clm after OHE
				
				we get category directly which has high probability
				
				it focus on finding other categories so it gives all probability of every category
				
			3.Sparse Categorical Cross Entropy : Used for Multiclass classification	
				-Same as Categorical Cross Entropy
				we are getting index of max probable
				
				diffenernce between sparse categorical cross entropy and normal categorical cross entropy is:
					it does not focus on finding other categories
					just tell which category has hight output probability then it gives us that answer only
				
				Disadvantages:
					Loosing info about probability of other category
	
	
	Which loss function when to use:
			
		combinations
			
			Hidden layers       o/p layer      problem statment type      loss function
		   1.Relu                sigmoid         Binary classification     Binary Cross Entropy 
		   2.Relu				 Softmax          multi class              Categorical Cross Entropy/Sparse Categorical Cross Entropy Sparse Categorical Cross Entropy 
		   3.Rely/its varient    linear           Regression               MSE,MAE,Hubor Loss,RMSE
	
	Optimizer:
		
		1.Gradient Descent Optimizer
		2.Stichastic Gradient Descent Optimizer
		3.Mini Batcg SGD Optimizer
		4.SGD with Momentum Optimizer
		5.Adagrad and RMSPROP Optimizer
		6.Adams Optimizer Optimizer
		
		1.Gradient Descent:
			
			we should focus on Epochs and iteration specificaly for cost function
				
			suppose in dataset i have 1000 datapoints in forward propogation we take all datapoints we comput its ŷ in loss function
			 but in cost function it take all datapoint at onece for backword propogation 
			
			Advantage:
				-Conversions will happen coz of wt updation formula
			
			Disadvantage:
				-Require huge resource like RAM and GPU
			
Important Terms:
	Weights in Neural networks
		Importance of weight is that helps to pass signal to neuron
	