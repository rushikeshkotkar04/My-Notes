Mathematics:
	Statastics and it applications:
		
		Types:
			
			1.Descriptive:
				It consists of organizing and summersizing the data
				
				Techniques:
					1.Measure of central tendancy
						Mean
						Median
						Mode
					
					2.Measure of dispursion
						Varianance
						Standerd daviation
						
		
		
			2.Inferential/Conclusion:
				Suppose u collect some data we make some conclusion using some experiments
				gives us some other data and that data called population data
				Exprimnets:
					Z-Test
					T-Test
		
		Population(symbol: N) and sample(symbol :n):
			
			consider we have a village of 100 people and i want to know what is the average weight of peoples in the village
			
			for
				-Population : we have to visit all peoples place and take weight of people
				
				as it is hard to do so we can give sample like some people and get there and conclude on it
				
			
		Measure of central tendancy
			1.Mean (symbol:μ)
				-Average of all elements in data set
				
			
			2.Median:
				Sort the data and gets middle value if odd numbers of element there else if even no is present
				then take middle to after sorting and the averatge of those two element will be median
				
				-Helps to fight against outliers
				
				
			3.Mode:
				
				Most frequent no present
		
		
		
		Measure of dispursion:
			dataset={1,1,2,3}
			Varianance(σ²):
				-It tells you how far the data points are from the mean, on average, but in a squared sense.
				
				Formuulla:
					σ²=sum(((dataele-mean)^2)/total elements))
				
			
	
		
	Equation of line,3D plane and hyperplane(n diamension)
	
	

Data analysis with python:
	
	1.Numpy:
		A mathematical lib helps to perform computation also help with matrix and list and other maths operations on data.
		
		everything in numpy array(np.array(list of elements)) are element wise
		
	
	2.Pandas:
		
		
		1.df[clname]     --> give clm its dtype is series
		2.df.loc[index]  --> give rows 
		3.df.iloc[index] --> give clm 
	
	
	Feature engineering:
		
		1.Handling missing values:
			
			Mechanism why missing values happens:
			
				1.Missing completly at random (MCAR):
					Without reason it happens like might be some error at entry end
				
				2.Missing at Random (MAR):
					Depends on other data why this data is missing
					Because there no data present for other entity
					
					like if you dont have mobile then u wont have mobile number
				
				3.Missing data not at random(MNAR):
					missing is because of some other reason or dependant
				
				
			Techniques:
				
				1:Delete the rows where missing values:
					Disadvantage: We can loose useful data also
					
					syntax to do:
						df.dropna()
				2:Delete clms has Misisng values:
					Disadvantage:Can delete useful clm
						
						syntax to do:
							df.dropna(axis=1) 
								(axix=1 means clm by default its 0 means rows)

				
				Imputation Techniques to handle missing values

					1-Mean Value imputation
						-Replace missing values with the mean of clm values
						-If we have different kind of distrubuted like righr skewed or left skewed it wont work

					2-Median Value imputation
						-It works also if we have different kind of distrubuted
						-It also helps to ahndle outliers in dataset
						
					3.Mode Imputation techinque
						-It is used for categrical clm has null values
					
					4.Random value :
						Randmoly picking value and fill but every run it can be changed
					
		
		2.Handling imbalanced Dataset:
			-In categorical database there might be chance
			some data is present a lot and some less so it get imbalalnced
		
			Techniques:
				1.Upsampling:
					Incease the datapoint from minorities
					
					cons:
						varience is not increaing
				
				2.DownSampling:
					Reduce the datapoints from majority
				
				3.SMOTE (Synthetic minority oversampling Technique)
					-Interpolating techinque
					-Create or join two nearest point and create new datapoint there
					-So variance can be increased
		
		3.Handling outlieres
			
			It can be done by box plot
			-We have to use 
			5 number summary is imp to get box plot:
				min val,max val,median,Q1,Q3,IQR
		
		4.DATA ENCODING
			we can use sklearn.preprocessing
			-As machine only knows the 0/1 language we have to convert categorical data into 0/1 format
				Types:
					1.Nominal or ONE HOT ENCODINH
						-Convert into binary format 1/0
						-It creates the num of clm as many diffent values present there and assign 1 if that clm value there else 0
						-Should not use if there are more values present then it will create as many clm there should not use in those cases
						-Sparse matrix
					2.Label and ordinal Encoding:
						-Assigns unique lebel to each categories
					3.Ordinal encoding:
						-Use to encode data in way of ranking or order
						example:
							Hightschool,College,Graduate,Post-Graduate:
							It will convert them in form like:
								
								Hightschool:1
								College:2
								Graduate:3
								Post-Graduate:4
								
					4.Target Guided Ordinal Encoding:
						Technique which is used to encode categorical variable based on there target variable
						Useful when there are lot of unique categories
					
			
	EDA:
		To be learned
	
	

Machine learning:
	
	Types:
		1.Supervised ML:
		
			Task: To predict house price
			
				we have dataset with features like
					size of housemNo of rooms in housemPrice of house
				
			In supervised ML we devide into Independant/Input and Dependadant/Output feature
			
				Types:
					Regression: Output values will be contineous
					classification:Output are in categories
			
			
			Algorithms used in supervise:
				Linear regression(Regression)
				Ridge and Lasso(Regression)
				Elastic Net(Regression)
				Logistic Regression(classification)
				Decision Tree (both Regression and Regression)
				Random forest (both Regression and Regression)
				Adaboot (both Regression and Regression)
				XgBoost (both Regression and Regression)
				
		
		2.Unsupervised ML:We create clustors based on situation
			
			Task:Customer segmentation
			we do not know the output feature we need to predict anything
			
			features like:
				Salary   spending score
				20000        9
				45000        2
				-- -         -- 
				-- -         --
				-- -         --
				
			lets suppose i launched a product and want to send mail dsicount coupon via main we can create clustor
			
			Algorithms used in Unsupervised:
				K Means
				Hirarichel mean
				DBScan clustering
		
		Reinforcement learning:
			Application will learn by itself by getting some amazin rewards
			
			eg:
	

Natural Language processing:
	
	Step 1:
		Text preprocessing 1: -> Clean the text data 
			
			Techniques:
				Tokenizations
				lemmitization
				stemming
				stopwords
				
	Step 2:
		Text preprocessing 2: -> Convert Text data into vectors
	
			Techniques:
				BOW (Bag of words)
				TF-IDF
				UniGrams
				BiGrams
		
	Step 4:
		Text preprocessing 3: -> Convert Text data into (vectors advance)
	
			Techniques:
				Word2vec
				AvgWord2Vec
	
	Step 4:
		Deep Learning technique used for handling text related use cases like Spam or not yes or no

			Techniques:
				RNN
				LSTM RNN
				GRU RNN
	
	Step 5:
		Text preprocessing 4: -> Convert Text data into (vectors advance)
	
			Techniques:
				word embading
	
	Step 5:
		Transformer
	
	Step 6:
		BERT
	
	
	
	
	
	-Tokenizations:
		
		-A process to convert paragraph into tokens(Sentences)
		and sentences into words
		
		Corpus:     Also calls paragraph
		Documents : Sentences
		vocabulary: Unique words
		Words:      Words same words
		
		we can use NLTK to perform tokenization
		
		for words we use : from nltk.tokenize import word_tokenize
		for sentence we use : from nltk.tokenize import sent_tokenize
		
	
	-Stemming -Text Preprocessing:
		Stemming is process to its stem that 
		stem mean deleting prefix/sufficx of word to convert into same root word
		it may be meaningful or not
		
		example:
			Task : comments on product is a positve review or negative one(classification)
			
			example:
				we have words like eating ,eat,eaten all convert into eat
			
				
			
		Stemming techniques:
			1.PorterStemmer
			2.RegexpStemmer
			3.SnowballStemmer: Better than Porter stemmer (Mostly used)
		NLTK usase RegexpStemmer class
	
	-Lemmatization: 
	
		-It makes word meaningful after stem it so it makes root word better
		-Techniques:
			WordnetLemmatizer:
				it uses morphy() function to the wordNet CorpusReader to find lemma
			
		in lemmitization we have pos='n' (pos-> Parts of speech)
		it means noun
		for 
		noun->n
		verb->v
		adjective->a
		adverb->r
		
		
		by default pos tag is n (noun)
		-It is slower than stemming
	
	-Stopwords:
		The words like The,Have,And,Of,Their,Two,Why.... are stop words
		wont play imp role in the our logic so these are stop words
		
		we can get all stopwords by using nltk
	
	
	-Converting text into vector:
		
		Technique:
			1.ONE HOT ENCODING
			2.Bag of words (BAG)
			3.TF-IDF
			4.Word2vec
			5.AvgWord2Vec
		
		
		1.ONE HOT ENCODING
			- It creates a clm for every unique word and create matrix
			per documnet as per word present as 1 or if not present then 0
		
		2.Bag of words:
		
			Dataset
			
			  Text                  		o/p
			1.He is a good boy       		  1
			2.she is a good girl     		  1
			3.Boy and girl are good  		  1
			4.Boy is good and girl is good    3
			
			
			Step by Step impl of bag of words:
				Dataset -> convert all the words to lower case -> 
				remove stopwords -> calculate uniqe words with there frequency ->
				keeping words based top most frequence keep in feature ->
				it gets convert into vector if present then 1 or else 0 
				
			step 1: Lower and remove stop words dataset become:
				
				1->good boy
				2->good girl
				3->boy girl good
				4->boy good girl good
			
			step 2: Calculate uniqe words with there frequency
				
				words/vocabulary          frequency
				good                        5
				boy                         2
				girl                        2

			step 3: Apply Bag of Words (keeping words based top most frequence keep in feature)
			
				It creates vector using uniqye word as feature/clm
				and document/corpus/sentence becomes in dataset above mentioned is like
					
					Binary bag of words if present 1 else 0
					    good  boy  girl    o/p
					
					1->  1     1    0       1
					2->  1     0    1       1
					3->  1     1    1       1
					4->  1     1    1       1
					
					
					Normal bag of words it has the frq of word present in it
					
						good  boy  girl    o/p
					
					1->  1     1    0       1
					2->  1     0    1       1
					3->  1     1    1       1
					4->  2     1    1       1
					
					the count wise store the count if each feaure in word
					
			 and after all this this will be trained by ML/DL model
			
			Advantages  :
				-Easy to impl and intitive
				-Input is fixed (helpful for ML algos)
				
			Disadvantage:
				-Sparse matrix or array is still there and it leads to overfitting
				-Ordering of words in vector changed so meaning of word gets changed
				-Out of vocabulary
				-Semantic meaning is still not getting captured
			
		-N-Gram:
			-Combination of words
			
			s1-> The food is good
			s2-> The food is not good
			
			vocabulary :
			
							UniGram      		  BiGram
						food  not  good     food+good   food+not  not+good
				s1->	  1    0    1          1          0         0   
				s2->      1    1    1          0          1         1
			
		
		-TF-IDF:(Term frequence -Inverse document frequency)
			
			Formula for tem frequence:
					
				term freq(TF)=No of repetation of words in sentence/no words in sentence
				
				IDF=  log base e (No of sentence/no od sentences containing the word)
				

				
		
			
			 Text                  		o/p
			1.He is a good boy       		  1
			2.she is a good girl     		  1
			3.Boy and girl are good  		  1
		
			
			1->good boy
			2->good girl
			3->boy girl good
		
			
			calculating term freq(TF) with formula
		
									sentences 
				vocabulary		1      2      3
				
				good            1/2    1/2    1/3
				
				boy				1/2     0     1/3
				
				girl			0      1/2    1/3
				
			
			colculating INVERSE DOCUMENT FREQUENCY (IDF)
			
			
									
				vocabulary		IDF
				
				good             log base e(3/3)=0				
				
				boy				 log base e(3/2)      
				
				girl			 log base e(3/2)
		
		
			we multiply we multiply every TF sentence with everyother IDF
			every row rp everyother row in both TF and IDF formulla
			
			final TF-IDF
			
			                    words
			sentances      good             boy                       girl
				1            0          1/2*log base e(3/2)             0  
				2            0                0                       1/2*log base e(3/2)
				3            0          1/3*log base e(3/2)           1/3*log base e(3/2) 
				
			
			
			Advantages:
				
				-Intutive
				-Fixes size of fetures(Vocabulary size)
				-Word importance is getting captured:
					-if word is present in all the sentenes then it is given a less importance
					-If word is  not in every sentence the we have to give some value to them
			
			Disadvantage:
				-Sparsing is still exists
				-Out of vocabulary
		-Word embedding:
			-a technique that represents words as numbers so that machines can understand them
			-it is a term representation of words for text analysis ,Typically in the form of realvalues vector that encodes
			the meaning of the words such that the words that are close in the vector space are expected to be similar in the menaing.
			
			-in short it is just relationship between two words
			-It converts word into vector and plot on graph checj the distnce between two words 
			-simillar/same meaning words are close to each other and opposite words are away in space of vector
			
			example:
			
				there two words 1.Happy 2.Excited
					-both are same
				but suppose there are words like 1.Happy 2.Angry
					-Both are different
				
			
			there two types of word embading technique
			1.Count or frequency
				types:
					1.OHE
					2.BOW
					3.TF-IDF
			
			2.Deep learning trained model
				types:
					1.Word2vec
						types:
							1.CBOW (continuos bag of words)
							2.Skipgram
							
		
		-Word2Vec:
			-It is trained on 3 billion words by google
			-Value range in between -1 and 1 
			
			
			Advantages of word2 vec:
				-Instead of sparse matrix we get dense matrix
				-Symantic information will get captured (simmilar word captured like synonyms or asynonims)
				-Fixed set of diamensions so we are not dependant on vocabulary size
				-Out of vocabulary problem also solved as we are capturing all info like symantic information and all
				
				
				
			
			Technique for NLP,uses Neural netwok model to learn work associate from large corpus/sentence
			once trained it can detect synanous word or suggest additional words for partial sentence
			
										vocabulary words :
				feature 		|	Boys    Girl    King    Queen    Apple     Mango
				representations |
				                |    -1      1      -0.92     +0.93   0.01      0.05
				Gender          |    0,01    0.02    0.95     +0.96  -0.02      0.02
				Royal           |    0.03     0.02   -0.75     +0.68   0.96      0.96
				Age             |    ..................................................
				.               |    ..................................................
				.               |    ..................................................
				.               |    ..................................................
				nth             |    ..................................................
				
			-Every word in vocabulary converts into feature representation
			-Means each and every vocabulary words makes/creates it own vector
				of feature representation and create its vector it can be different diamentions
			
			Above table has random values but in real the equatin like
			
				
				Kng-Boy+Queen=Girl
			
			we can you 2dVector of a words also like following
			
				King=[0.95,0.96]    Man=[0.95,0.98]
				Queen=[-0.95,0.96]  Women=[-0.94,-0.96]
				
					Kng-Man+Queen=Women
				
				we can have cosine similarity:
				
				  we can use distance formula to chq how away two words on plane
				   the formula is :
						
						distance=1=cos(θ) (θ is angle between two points)
					
					if distance betweentwo word is 0 then word is same
				
			
			
			
			1.CBOW(Continous bag of words):
				
				suppose we have corpus/sentance or paragraph:
				
					Microsoft company is related to data science
					
					
					
					we select as window size ,
					so we sleect window size count of words
					the we select center word
					so input is left side of middle word and right side 
					and middle is output 
					then we move window with one word and select middle and have input and ouput same types
				
				C-BOW: is fully connected neural network
				CBOW -> If we have small dataset we should apply
				
				how to make it more accurate:
					Increase the training data
					Increase the Window size
			
			2.Skipgram:
				
				suppose we have corpus/sentance or paragraph:
				
					Microsoft company is related to data science
				
				everything will be same as CBOW as window size and all but 
				juts input gets as output and output as input
				
				CBOW -> If we have small dataset we should apply
				
				how to make it more accurate:
					Increase the training data
					Increase the Window size
			