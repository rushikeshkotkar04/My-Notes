GenAI:
	-Models used in Deep Learning
		1.Descriminative Models:
			-Tasks like classification,prediction,regression
			-Trained on labeled dataset
			
			
		2.Genrative AI:
			-Tasks line generating new data 
			-generate new data trainied on Huge dataset
			
			-Models use in GEN AI:
				1.LLM: Large language model
				2.LIM:Large Image model
		
	-LLM Models:
	
	
		
RAG (Retrieval Augmented Generation):
	-If we have Data Source eg PDF:
		-If we wanna make PDA as Documented QnA Application
	
	Modules to create the Applications like this:
		1.Module 1
			1.Data injection:
				-To get data from various sources like
				1.PDF,2.URLs,3.JSON,4.Imgs,5.Videos
			
			2.Split/Devide Data into smaller chunks
				-Split into:
					1.Text
					2.Documented
				-Some LLMs model has limitation on context size that why it is imp
				
				-Techniques to split data:
					1.
			
			3.Embde(Convert text to vectors)
			
			4.Store the Converted Vectors from text into store:
				-VectorStore DB
					1.FAISS DB
					2.CHROMADB
					3.ASTRADB
			
			
		2.Module 2 (Retrive)
			-Retrieval chain
				-A interface to query VectorStoreDB
			
		
		
LangChain ecosystem:
	-A framework to use to build GEN AI Application with any LLM model
	-All model in langChain are open source
	-LangSmith:
		-It has LangSmith help to do MLOPs things eg Debugging,Playground,evaluation,Annotaion,Monitoring
	-LangServe:
		Create chain as RestAPI and helps to tasks like Deployment
				

LangChain:
	-Imp Component of LangChain:
		-Data Injection
			1.Document loader:
				-Different ways to load data from various sources
			
		-Splitter:
			-we can split by:
				-Recursively by characters
				-by Htmlheader
				-Json
				-Charecher Text
				
					-Sample single systax here we can use more
						-Syntax:
							text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)
							final_documents=text_splitter.create_document(var_name)
							final_documents=text_splitter.split_documents(docs)
					
		-Embedding:
			-Converting text into vectors
			Ways To do Embed texts to vectors:
				1.OpenAi:
					syntax:
						-we need langchain-openai for openai api key
						from langchain_openai import OpenAIEmbeddings
						embeddings=OpenAIEmbeddings(api_key=OPEN_API_KEY,model="text-embedding-3-large")
						print(embeddings)
						
						text="This is tutorial in OPENAI embeddings"
						embeddings.embed_query(text)
						
						
				2.OLLAMA
				3.Hugging Face
		
		
		-Retrieval chain/documnet chain:
			-A interface ,Helps to retrive from vector Store and give output
			-we can use the retreiver to dynamically select the most relevant documents and pass 
				those in for a given question
			-Synatx
				-from langchain.chains.combine_documents import create_stuff_documents_chain
				-create_stuff_documents_chain is to give a paragaph or stuff in the document where we can get data
			
			-Flow:
				Input->Retriever->VectorStoreDB
			
			create_stuff_documents_chain: 
				This helper reads the pages you give it and uses the LLM (like ChatOpenAI) to write an answer based on those pages.
		
			create_retrieval_chain:
				This helper combines the library (your vector database, like Chroma or Pinecone) with the page-reader helper to first find the right pages and then answer the question.
LLM:
	-Large language model
	-We have top import model
	llm=ChatOpenAI(model=model_name)
	input=input()
	llm.invoke(input)

Prompt template:
	-How you want LLM model to behave
	-In short setting up the rule how it behaves to give output
	from langchain_core.prompts import ChatPromptTemplate
	
	-Syntax:
	
		prompt=ChatPromptTemplate.from_messages(
		  [
			("system","Your are an expert AI engineer.Provide me answers based omn the question"),
			("user",f"{input}")
			
		  ]
		)
		
		-How to make sure our LLM works
		
		
		-For own chat prompt template
			-from langchain_core.prompts import ChatPromptTemplate
	
	
	
	-How to use prompt template with llm model:
		-now it chains to the llm
		-Syntax:
			chain=prompt|llm
			response=chain.invoke({"input":"Can you tell me LangSmith"})
			print(response)
			
		-in this syntax the value of input goes in the input filed of the prompt template
		-now it first goes to prompt an dthen goes to llm like that


-Output parser:
	-To display msg
	
	1.StrOutputParser:
	
		from langchain_core.output_parsers import StrOutputParser
		output_parser=StrOutputParser()
		chain=prompt|llm|output_parser
		response=chain.invoke({"input":"Can you tell me LangSmith"})
		print(response)
	-we made a chain of where it is like our input goes fron the first then its output go to second and then thord and go on


-Message History : 
	-To make model stateful
	-To store privous convo
		
		
	## Message History : To make model stateful
	from langchain_community.chat_message_histories import ChatMessageHistory
	from langchain_core.chat_history import BaseChatMessageHistory
	from langchain_core.runnables.history import RunnableWithMessageHistory

	store={}
	def get_session_history(session_id:str)->BaseChatMessageHistory:
	  if session_id not in store:
		store[session_id]=ChatMessageHistory()
	  return store[session_id]
	  
	with_message_history=RunnableWithMessageHistory(model,get_session_history)
	
	
	config1={"configurable":{"session_id":"chat2"}}

	resonse=with_message_history.invoke(
	  [
		HumanMessage(content="Whats my name?"),
	  ],
	  config=config1
	)
	resonse.content




tream_messages:
	To let know how many last messages to take forward else delete all from history
	we need RunnablePassThrough class to achieve this

temerature:
	we set it into 0-1
	-creativity depends on value
	


TOola sna Agents:
	Tools to do a specific task and Agent is like to find out the appropriate tool for that task


	In super short:
	Tools = Do the task

	Agents = Figure out what task to do, pick the right tool, and do it

	So an Agent is like a smart assistant that:

		Understands the goal

		Chooses the right tools 

		Uses them in the right way 
	
	
	How to create tools:
		Step1: Import Query Runner or specific thinhs
			from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun
			from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper
		
		
		Step2.1:Using InBuilt tools
			Create api wrapper of the tool and the use that wrapper for query run like following
			
			## Used the inbuilt tool of wikipedia
			api_wrapper_wiki=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)
			wikipedia=WikipediaQueryRun(api_wrapper=api_wrapper_wiki)
			wikipedia.name
			
			## Used inbuit tool for Arxiv
			api_wrapper_arxiv=ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=250)
			arxiv=ArxivQueryRun(api_wrapper=api_wrapper_arxiv)
			arxiv.name
			
			we can also create list of tools for user just to iterate
			like:
				tools=[wikipedia,arxiv]
			
		Step2.2:Create custom tools
			same like we split data and store in vectorDb
			also we have to convert it into tools
			we have to use retriever of the vectorDb and
			using langchain.tools
			retriever=vectordb.as_retriever()
			retriever
			from langchain.tools.retriever import create_retriever_tool
			retriever_tool=create_retriever_tool(retriever,"Langsmit-search","Search any info about langsmit")
			
			
		Step3:Create agent
			Agent is for run the appropriate tools 
			## Agents
			from langchain.agents import create_openai_tools_agent
			agent=create_openai_tools_agent(llm,tools,prompt)
			agent
		
		Step4:Run agent
			we use 
			agent.invole(query)
			
			for running and getting output

Summerize Text:
	Ways:
		1.Stuff:
		2.Map-Reduce: For larger files 
		3.Refine:
	
	1.Stuff Documents chain summerization:
		Most basic type of summerizarion technique
		we user all data as input to the prompt and pass to the prompt
		pass that prompt to the llm and get summerization
		
		problems:
			1.if we have 10 diffent documents in a pdf all gets compbined and summerize it that is problem
			
			also if combined doc gets very big cant give to LLM because of context size limitations of LLM model
	
	2.Map-Reduce summerization technique:
		Better than stuff Document
		
		-instead of combine all documents and give to LLMs
			we devide all insto smaller chunks and pass into a prompt template and get the summerize
			and get each part sumeeries
			we combune all summeries and get final summery
	
	3.Refine Summerization techinque:
		as document is in chunks
		we take single chunk pass to LLM for summerization
		for second chunk before sending this to the prompt
		we take befor summerization of last chunk and pass with second chunk to LLM
		like this we do with all chunks



FineTuning:
	
	Different modules:
		1.Quantization 
		2.LORA 
		3.QLORA
	
	1.Quantization:
		-Conversion from higher memory format to a lower memory format
		-Imagine we have model called Llama2-70b 
		
		it has 70 billions parameters so it is not  possible to get in laptop and use it
		as it has lot of parameters hard to use as space
		suppose it has fp-32 / 32 bit each paramters  so it is too much
		so we do use quantization to convert that big number in smaller like fp-32/32 bit to fp-8/ 8 bit so easy to use for works
		
		
		1.fully precision/Half precision: : Data-> wts and parameters
			half precision :
				when we convert one size to its half during quantization
				
				example:
					fp-32 / 32 bits to fp-16 /16 bits
					
					
		2.Callibration:	Model Quantization ->Problems
			-In short it is simply converting values from high level to lower for use 
			How we convert fp-32/32 bits into fp-8 or 8 bits
			what mathematical formullea is required for it
			
			Types:
				1.Symmetric Quantization:
				2.Asymmetric Quantization
			
			1.Symmetric Quantization:
				1.Symmetric unsined int8 Quantization
					eg:
						(32-bits numbers)
						Numbers=[0.0,0.1 ..... 1000] 
						
						for convert inti unsined int 8 bit(0-255)
						in short convert numbers ranging 0-1000 to 0-255
						
						
						from 0-1000 to 0-255
						
						formulla to convert 32 bit to 8 bits is
							the number wanna convert devided by scaleFactor with rounded value
							
							formulla= Round(num/scaleFactor)
							
							scaleFactor=(Xmax-Xmin)/(Qmax-Qmin)
							
							
						
							
								0-1000 = X (supposed)
								0-255  = Q  (supposed)
								
								scaleFactor=(1000-0)/(255-0)=1000/255=3.92
							
							Now converion:
								if wanna convert 32 bits of 250 into 8 bits
								
								quantizeValue=round(250/scaleFactor)=round(250/3.92)=round(63.7777)=64
								
								
								
								
			2.ASymmetric Quantization:
				1.ASymmetric unsined int8 Quantization from signed integer 32 bits
					eg:
					(32-bits numbers)
					Numbers=[-20.0,.........1000] 
					
					covernt to unsined int 8 
					Numbers are not evern distrubuted it as in negative it might be left skewed or right skewed
					
					as it wont work same scaleFactor formula it wont be that correct as negative values can make changes
					
					so in this we have to have ZeroPoint
					
					so with same formula we have to get Zeropoint in it and add then use same process as symmetric quantization
								
			
		3.Modes of quantization
			1.Post training quantization
			2.Quantization aware training
			
			1.Post training quantization:
				suppose we have already pretrained model
				
				on pretrained model we apply callibration we takes wts data and then convert into quantized model
			
			
		
		FP-fully precision /single precision
		FP-32  (we can say floating point number)
		
		
	2.LORA
	3.QLORA

	
			
	
	